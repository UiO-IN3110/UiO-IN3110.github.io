{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0d5443",
   "metadata": {},
   "source": [
    "# Pandas - Data Analysis with Python\n",
    "(last update 20/10/22 still work in progress)\n",
    "\n",
    "Pandas is a high-performance, easy-to-use data structures and data analysis tools.\n",
    "\n",
    "<img src=\"images/pandas.jpg\" style=\"width: 500px;\"/>\n",
    "\n",
    "**There is abundance of data and we should/need to make sense of it**\n",
    "- Smart devices, Strava, ...\n",
    "- [SSB](https://data.ssb.no/api/) Statistics Norway *\n",
    "- [EuroStat](https://ec.europa.eu/eurostat/web/population-demography/demography-population-stock-balance/database) Statistics Europe *\n",
    "- [Kaggle](https://www.kaggle.com/datasets/) Data Science compatitions*\n",
    "- [Quandl](https://www.quandl.com/) for finances\n",
    "- [Yr](http://om.yr.no/verdata/free-weather-data/)\n",
    "- [Oslo Bysykkel API](https://developer.oslobysykkel.no) **\n",
    "\n",
    "(We will work with these ones today in lecture* or exercises**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797e613",
   "metadata": {},
   "source": [
    "## What is Pandas good for?\n",
    "\n",
    "Working with (large) data sets and created automated data processes.\n",
    "\n",
    "Pandas is extensively used to prepare data in data science (machine learning, data analytics, ...)\n",
    "\n",
    "Examples:\n",
    "- Import and export data into standard formats (CSV, Excel, Latex, ..).\n",
    "- Combine with Numpy for advanced computations or Matplotlib for visualisations.\n",
    "- Calculate statistics and answer questions about the data, like\n",
    "- What's the average, median, max, or min of each column?\n",
    "  - Does column A correlate with column B?\n",
    "  - What does the distribution of data in column C look like?\n",
    "- Clean up data (e.g. fill out missing information and fix inconsistent formatting) and merge multiple data sets into one common one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51a27b",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "* Official Pandas documentation: http://pandas.pydata.org/pandas-docs/stable/tutorials.html \n",
    "* Pandas cookbook: http://pandas.pydata.org/pandas-docs/stable/cookbook.html\n",
    "* Wes McKinney, Python for Data Analysis \n",
    "![Python for Data Analysis](images/python_for_data_analysis.gif \"Python for Data Analysis\")\n",
    "* [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas *(We follow Chapter 4 in this lecture)*\n",
    "<img src=\"./images/PDSH-cover.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "## This lecture\n",
    "- Part1 Introduction to Pandas\n",
    "- Part2 Hands on examples with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e43d5e",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "- If you have Anaconda: Already installed\n",
    "- If you have Miniconda: \n",
    "      `conda install pandas`\n",
    "- If you have your another Python distribution: \n",
    "      `python3 -m pip install pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a60315",
   "metadata": {},
   "source": [
    "Let's dive in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2044821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36309d38",
   "metadata": {},
   "source": [
    "## Pandas `Series` object\n",
    "`Series` is 1d series of data similar to `numpy.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series([4, 5, 6, 7, 8])\n",
    "series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(series.values.dtype, series.index.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93d0a6",
   "metadata": {},
   "source": [
    "We see that `series` are indexed and both `values` and `index` are typed. As we saw with numpy this has performance benefits. The similarity with `numpy.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.power(series, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07982899",
   "metadata": {},
   "source": [
    "However, the indices do not need to be numbers (and neither need to be the values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(range(65, 75))\n",
    "index = [chr(v) for v in values]\n",
    "series = pd.Series(values, index=index)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34188b",
   "metadata": {},
   "source": [
    "The `series` than behaves also like a dictionary, although it supports fancy indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "series['A'], series['D':'H']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d8317",
   "metadata": {},
   "source": [
    "Continuing the analogy with dictionary a possible way to make `series` is from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d106c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Washington': 'United States of America',\n",
    "    'London': 'Great Britain',\n",
    "    'Oslo': 'Norway'\n",
    "}\n",
    "series = pd.Series(data)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f932cf4",
   "metadata": {},
   "source": [
    "Note that for values that are amenable to `str` we have the string methods ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca733e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "series.str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6d660",
   "metadata": {},
   "source": [
    "... and so we can for example compute a mask using regular expressions (here looking for states with 2 word names) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = series.str.match('^\\w+\\s+\\w+$')\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8efdd8",
   "metadata": {},
   "source": [
    "... which can be used to index into the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192df8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "series[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb652753",
   "metadata": {},
   "source": [
    "Some other examples of series indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "(series[0], series['London':'Oslo'], series[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661dbb9c",
   "metadata": {},
   "source": [
    "We can be more explicit about the indexing with indexers `loc` and `iloc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "(series.iloc[0], series.loc['London':'Oslo'], series.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7990b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "series[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67195b07",
   "metadata": {},
   "source": [
    "But why does this matter? Consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf685e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky = pd.Series(['a', 'b', 'c'], index=[7, 5, 4])\n",
    "tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f705d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate that we can sort\n",
    "tricky = tricky.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are refering to the value at row where index = 4 and values where at first though second row!\n",
    "(tricky[4], tricky[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fdb4c",
   "metadata": {},
   "source": [
    "## Pandas `DataFrame` object\n",
    "Two dimensional data are represented by `DataFrame`s. Again *Pandas* allows for flexibility of what row/columns indices can be. `DataFrame`s can be constructed in a number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spreadsheet(nrows, ncols, start=0, cstart=0, base=0):\n",
    "    data = base + np.random.rand(nrows, ncols)\n",
    "    index = [f'r{i}' for i in range(start, start+nrows)]\n",
    "    columns = [f'c{i}' for i in range(cstart, cstart+ncols)]\n",
    "    # c0 c1 c2\n",
    "    #r0\n",
    "    #r1\n",
    "    return pd.DataFrame(data, index=index, columns=columns)\n",
    "\n",
    "data_frame = spreadsheet(10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e26647",
   "metadata": {},
   "source": [
    "For large frame it is usefull to look at portions of the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spreadsheet(10_000, 4)\n",
    "print(len(data_frame))\n",
    "data_frame.head(10)   # data_frame.tail is for the end part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0cb37",
   "metadata": {},
   "source": [
    "We can combines multiple series into a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1249143",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ('Min', 'Ingeborg', 'Miro')\n",
    "nationality = pd.Series(['USA', 'NOR', 'SVK'], index=index)\n",
    "university = pd.Series(['Berkeley', 'Bergen', 'Oslo'], index=index)\n",
    "\n",
    "instructors = pd.DataFrame({'nat': nationality, 'uni': university})\n",
    "instructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1637cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(instructors.index, instructors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35278bee",
   "metadata": {},
   "source": [
    "Anothor way of creating is using dicionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.DataFrame({'country': ['CZE', 'NOR', 'USA'], \n",
    "                  'capital': ['Prague', 'Oslo', 'Washington DC'],\n",
    "                  'pop':     [5, 10, 300]})\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa86352e",
   "metadata": {},
   "source": [
    "When some column is unique we can designate it as index. Of course, tables can be stored in various formats. We will come back to reading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTex\n",
    "import re\n",
    "\n",
    "fi = f.set_index(\"country\")\n",
    "\n",
    "tex_table = fi.style.to_latex()\n",
    "# To ease the prining \n",
    "for row in re.split(r'\\n', tex_table):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "tex_table = fi.to_csv(sep=\";\")\n",
    "# To ease the prining \n",
    "for row in re.split(r'\\n', tex_table):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf2fa0",
   "metadata": {},
   "source": [
    "Going back to frame creation, recall that above, the two series had a common index. What happens with the frame when this is not the case? Below we also illustrate another constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = {'Miro': 'SVK', 'Ingeborg': 'NOR', 'Min': 'USA'}\n",
    "university = {'Miro': 'Oslo', 'Ingeborg': 'Bergen', 'Min': 'Berkeley', 'Joe': 'Harvard'}\n",
    "office = {'Miro': 303, 'Ingeborg': 309, 'Min': 311, 'Joe': -1}\n",
    "# Construct from list of dictionary using union of keys as columns - we need to fill in values for some\n",
    "# NOTE: here each dictionary essentially defines a row\n",
    "missing_data_frame = pd.DataFrame([nationality, university, office], index=['nation', 'uni', 'office'])\n",
    "missing_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make it look better\n",
    "missing_data_frame = missing_data_frame.T\n",
    "missing_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa72076",
   "metadata": {},
   "source": [
    "The missing value has been filled with a special value. Real data often suffer from lack of regularity. We can check for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c9a7e",
   "metadata": {},
   "source": [
    "And the provide a missing value. Note that this can be done already when we are constructing the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame.fillna('OutThere')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b8735",
   "metadata": {},
   "source": [
    "Or discard the data. We can drop the row which has NaN. Note that by default any NaN is enough but we can be more tolerant (see `how` and `thresh` keyword arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29369536",
   "metadata": {},
   "source": [
    "Or we drop the problematic column. Axis 0 is row (just like in `numpy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ed810",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc317ef3",
   "metadata": {},
   "source": [
    "Once we have the frame we can start computing with it. Let's consider indexing first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0197212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific series\n",
    "missing_data_frame['nation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or several as a frame\n",
    "missing_data_frame[['nation', 'uni']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f5bbc",
   "metadata": {},
   "source": [
    "Note that slicing default to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will fail because no rows names like that\n",
    "missing_data_frame['nation':'office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that slice goes for rows\n",
    "missing_data_frame['Miro':'Min']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28179044",
   "metadata": {},
   "source": [
    "We can call *indexers* to rescue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame.loc[:,'nation':'uni']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9457d",
   "metadata": {},
   "source": [
    "Indexes can of course be more involed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame['office'] > 305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_frame.loc[missing_data_frame['office'] > 305, ['uni', 'nation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b5348",
   "metadata": {},
   "source": [
    "Next let's compute with the values in the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = spreadsheet(4, 4)\n",
    "np.cos(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26cc73f",
   "metadata": {},
   "source": [
    "Binary operation will align the frame - operation is valid only for they index-column pairs found in both and with will get missing for the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0557fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = spreadsheet(5, 6)\n",
    "f0 + f1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81d26c",
   "metadata": {},
   "source": [
    "Note that this is note unique to frames and works (unsuprisingly on series too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ef0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c00 = f0['c0']\n",
    "c10 = f1['c0']\n",
    "c00 + c10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199beb07",
   "metadata": {},
   "source": [
    "Reduction operators default to be computed for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811ffa9",
   "metadata": {},
   "source": [
    "For column wise reduction we need to specify the axis (`axis=1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc715e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.mean(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc28d95",
   "metadata": {},
   "source": [
    "## Pandas `Index` object\n",
    "\n",
    "In the previous usecase `f0 + f1` we have seen that operation is performed for _common_ keys - this suggests that we can do logical operations on indices. This is indeed the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "i0 = f0.index\n",
    "i1 = f1.index\n",
    "(i0.union(i1), i0.intersection(i1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00839931",
   "metadata": {},
   "source": [
    "Indexes can be explicitely constructed. For working with time series it is useful to index by time. Let's get timestamp indexing \n",
    "for every **D**ay between 2 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "time = pd.date_range(start=datetime.date.fromisoformat('2022-10-19'),\n",
    "                     end=datetime.date.fromisoformat('2022-12-20'), freq='D')\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd51c0d",
   "metadata": {},
   "source": [
    "We generate the corresponding data and illustrate some plotting capabilities of `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "values = np.sin(np.pi/10*np.arange(len(time)))\n",
    "\n",
    "time_series = pd.Series(values, index=time)\n",
    "time_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fda4e",
   "metadata": {},
   "source": [
    "Note that we are getting the nice xlabels for free!\n",
    "\n",
    "Time indexing allows us to do some fancy opearations. For example we can have a look at the signal for only the first day of the week (Monday?). For other functionality see `resample`, 'rolling' means on `windows` ets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series[time.weekday == 0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16bbe4",
   "metadata": {},
   "source": [
    "### Hierarchical indexing with MultiIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086fe54b",
   "metadata": {},
   "source": [
    "Multiindexing is a way to handle higher order tensors, e.g. f(x, y, color). It also allows for organizing the data by establishing hierarchy in indexing. Suppose we have RGB image of 5 x 4 pixels. We could represent this as a table \n",
    "with channel x and y coordinate columns and 5 x 4 x 3 rows. A representation more true to the nature of the data would be to have a \"column\" for each color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_index = pd.MultiIndex.from_product([['R', 'G', 'B'], [0, 1, 2, 3]], names=('color', 'cindex'))\n",
    "column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pd.DataFrame(\n",
    "    np.arange(60).reshape(5, 12),\n",
    "    index=pd.Index([0, 1, 2, 3, 4], name='rindex'),\n",
    "    columns=column_index\n",
    ")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ccbda0",
   "metadata": {},
   "source": [
    "We can have a look at the said \"flat\" representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4be873",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c855cf",
   "metadata": {},
   "source": [
    "Accessing the entries works as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2629d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image['R']                 # Column(s) where first multiindex is R\n",
    "image.loc[:, ('R', 0)]     # Column multiindex by R, 0\n",
    "image.loc[2, ('R', 1)]     # The entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58750e1",
   "metadata": {},
   "source": [
    "We can perform reduction operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.mean().mean(level='cindex')  # Unpack this - note the warning is pointing to a different way of aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba9217",
   "metadata": {},
   "source": [
    "## Combining datasets - appending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef8dd6",
   "metadata": {},
   "source": [
    "Extend `numpy.concatenate` to `pandas` objects. We can combine data from 2 frames if they have some \"axes\" in common. Let's start with 2 identical columns and do default vertical/row wise concatenation. Note that all the operations below create new `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique row indices\n",
    "d1 = spreadsheet(2, 2, base=0)\n",
    "d2 = spreadsheet(4, 2, base=10)\n",
    "print(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3342b89",
   "metadata": {},
   "source": [
    "What if there are duplicate indices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f47377",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = spreadsheet(2, 2, base=0)   # Has r0, r1\n",
    "d2 = spreadsheet(2, 2, base=10)  # Has r0, r1\n",
    "print(d1)\n",
    "print(d2)\n",
    "d12 = pd.concat([d1, d2])\n",
    "d12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1810ee3",
   "metadata": {},
   "source": [
    "We can still get values but this lack of uniqueness might not be desirable and we might want to check for it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bcf50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d12.loc[\"r0\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e82de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d2], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6da2c",
   "metadata": {},
   "source": [
    "... or reset the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ca597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62b7e1",
   "metadata": {},
   "source": [
    "A different, more organized option is to introduce multiindex to remember where the data came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "d12 = pd.concat([d1, d2], keys=['D1', 'D2'])\n",
    "d12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68182e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "d12.loc['D1', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81d0b1",
   "metadata": {},
   "source": [
    "For frames sharing row index we can vertical/column wise concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = spreadsheet(2, 3, cstart=0)\n",
    "d2 = spreadsheet(2, 3, cstart=3)\n",
    "print(d1)\n",
    "print(d2)\n",
    "pd.concat([d1, d2], axis=1, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dec62c",
   "metadata": {},
   "source": [
    "What if we are appending two tables with slighlty different columns. It makes sense that there would be some missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = spreadsheet(2, 2, cstart=0)   # Has c0 c1\n",
    "d2 = spreadsheet(2, 3, start=0, cstart=1)  # c1 c2 c3\n",
    "print(d1)\n",
    "print(d2)\n",
    "pd.concat([d1, d2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87ef7e",
   "metadata": {},
   "source": [
    "We see that the by default we combine columns from both tables. But we can be more specific using the `join` or `join_axes` keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c42fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d1, d2], ignore_index=True, join='inner')  # Use the column intersection c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f49ecd",
   "metadata": {},
   "source": [
    "## Combining datasets - use relations to extend the data\n",
    "For `pd.concant` a typical use case is if we have tables of records (e.g. some logs of [transaction id, card number], [transaction id, amount]) and we want to append them. A different of way of building up/joining data is by `pd.merge`. Consider the following \"relational tables\". From `nationality` we know that \"Min\" -> \"USA\" while at the same time `university` maps \"Min\" to \"Berkeley\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978bb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = pd.DataFrame({'name': ['Min', 'Miro', 'Ingeborg', 'Aslak'],\n",
    "                            'nation': ['USA', 'SVK', 'NOR', 'NOR']})\n",
    "print(nationality)\n",
    "university = pd.DataFrame({'name': ['Miro', 'Ingeborg', 'Min', 'Aslak'],\n",
    "                           'uni': ['Oslo', 'Bergen', 'Berkeley', 'Oslo']})\n",
    "print(university)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e143a",
   "metadata": {},
   "source": [
    "Note that the frame are not aligned in their indexes but using `pd.merge` we can still capture the connection between \"Min\", \"Berkely\" and \"USA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(nationality, university)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e8e1f",
   "metadata": {},
   "source": [
    "Above is an example of 1-1 join. We can go further and infer properties. Let's have a look up for capitals of some states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = pd.DataFrame({'nation': ['USA', 'SVK', 'NOR'],\n",
    "                         'city': ['Washington', 'Brasislava', 'Oslo']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ad58e",
   "metadata": {},
   "source": [
    "Combining `nationality` with `capitals` we can see that \"Ingeborg -> NOR -> Oslo\" and thus can build a table with cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc5545",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(nationality, capitals)   # Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7a1f2",
   "metadata": {},
   "source": [
    "Above the merge happened `on` unique common columns. Sometimes we want/need to be more explicit about the \"relation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e634498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table does not have the \"nation\" column\n",
    "capitals2 = pd.DataFrame({'state': ['USA', 'SVK', 'NOR'],\n",
    "                          'city': ['Washington', 'Brasislava', 'Oslo'],\n",
    "                          'count': [4, 5, 6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d67f8",
   "metadata": {},
   "source": [
    "We can specify which columns are to be used to build the relation as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have nation and state but they represent the same so drop\n",
    "pd.merge(nationality, capitals2, left_on='nation', right_on='state').drop(\"nation\", axis=\"columns\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949f8f6",
   "metadata": {},
   "source": [
    "Indexes for row can also be used to this end. Let's make a new frame which use `state` column as index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d87335",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals_ri = capitals2.set_index('state')\n",
    "capitals_ri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9a3be",
   "metadata": {},
   "source": [
    "It's merge can now be done in two ways. The old one ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c34155",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(nationality, capitals_ri, left_on='nation', right_on=\"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3755319",
   "metadata": {},
   "source": [
    "... or using row indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cac634",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(nationality, capitals_ri, left_on='nation', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1acb7",
   "metadata": {},
   "source": [
    "What should happen if the relation cannot be establised for all the values needed? In the example below, we cannot infer position for all the `names` \n",
    "in the `nationality` frame. A sensible default is to perform merge only for those cases where it is well defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a944126",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.DataFrame({'who': ['Ingeborg', 'Aslak'],\n",
    "                     'position': ['researcher', 'ceo']})\n",
    "\n",
    "pd.merge(nationality, jobs, left_on=\"name\", right_on=\"who\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf6600",
   "metadata": {},
   "source": [
    "However, by specifying the `how` keyword we can proceed with NaN for \"outer\" all the merged values or those in \"left\" or \"right\" frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(nationality, jobs, left_on=\"name\", right_on=\"who\", how=\"outer\")  # Inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a938bc",
   "metadata": {},
   "source": [
    "Finally, `pd.merge` can yield to duplicate column indices. These are automatically made unique by `suffixes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(jobs, jobs, on=\"who\", suffixes=['_left', '_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e7cb9",
   "metadata": {},
   "source": [
    "## Data aggregation and transformations\n",
    "\n",
    "We have previously seen that we can extact information about `min`, `max`, `mean` etc where by default reduction happend for each fixed column. Hierchical indexing was one option to get some structure to what was being reducued. Here we introduce `groupby` which serves this purpose too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'brand': ['tesla', 'tesla', 'tesla', 'vw', 'vw', 'mercedes'],\n",
    "                     'model': ['S', '3', 'X', 'tuareg', 'passat', 'Gclass'],\n",
    "                     'type': ['sedan', 'sedan', 'suv', 'suv', 'sedan', 'suv'],\n",
    "                     'hp'   : [32, 43, 54, 102, 30, 50]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4698b",
   "metadata": {},
   "source": [
    "Let us begin by \"fixing\" a table - say we want to capitalize the \"brand\" data - we can use `transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aede34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brand'] = data[\"brand\"].transform(str.capitalize)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff60de",
   "metadata": {},
   "source": [
    "Same idea done differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13153b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brand'] = data[\"brand\"].str.upper()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec618192",
   "metadata": {},
   "source": [
    "Finally, we look at `transform` of the table. Here we get access to the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641981e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    x[\"w\"] = x['hp']*0.7\n",
    "    return x\n",
    "\n",
    "data.apply(foo, axis=\"columns\")  # NOTE: the new index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1181c4",
   "metadata": {},
   "source": [
    "Now that we are happy with the table we now want to gather information based on `brand`. If we split by `brand` we can think of 3 (as many as unique `brand` values) subtables. Within these subtables we perform further operations/reductions - they values with be combined in the final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data.groupby(\"brand\", group_keys=True)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded12f61",
   "metadata": {},
   "source": [
    "Note that we do not get a frame back but instead there is a `DataFrameGroupBy` object. This way we do not compute rightaway which could be expensive. We can verify that it it's behavior is similar to the frame we wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1b3e1",
   "metadata": {},
   "source": [
    "The object supports many methods of the `DataFrame` so we can continue \"defining\" our processing pipiline. For example we can get the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabc66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"hp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12182183",
   "metadata": {},
   "source": [
    "And finally force the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"hp\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb3584",
   "metadata": {},
   "source": [
    "More complete description by `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"hp\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66f8e5",
   "metadata": {},
   "source": [
    "We can also use several aggregators at once by `aggregate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"hp\"].aggregate([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b1afe",
   "metadata": {},
   "source": [
    "### Pivoting\n",
    "Another way of getting overview of the data is by `pivoting` (somewhat like groupby in multiple \"keys\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View \"hp\" just as a function of brand where the remaining dimensions are aggregated\n",
    "data.pivot_table(\"hp\", index=\"brand\", aggfunc=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View \"hp\" just as a function of brand and type (leaving agg on models). WE have 2 types SUV and SEDAN\n",
    "data.pivot_table(\"hp\", index=\"brand\", columns=['type'], aggfunc=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in missing data\n",
    "data.pivot_table(\"hp\", index=\"brand\", columns=['model'], aggfunc=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6c821",
   "metadata": {},
   "source": [
    "## Hands on examples\n",
    "\n",
    "Let's explore some datasets\n",
    "\n",
    "__1. Used cars__\n",
    "\n",
    "This dataset is downloaded from [Kaggle](https://www.kaggle.com/datasets/tsaustin/us-used-car-sales-data?resource=download) and statistics on saled card (ebay?). More precisely, having read the data by built-in \n",
    "reader `read_csv` we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/used_car_sales.csv\", dtype={'Mileage': int, \"pricesold\": int})\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a6c6b",
   "metadata": {},
   "source": [
    "We can find out which manufacturer sold the most cars, by grouping and then collapsing based on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity check\n",
    "data.groupby('Make').size().sum() == len(data)\n",
    "# Unpack this\n",
    "data.groupby('Make').size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68628dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Make'].value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11df85",
   "metadata": {},
   "source": [
    "We might wonder how the price is related to the car attributes. Let's consider it first as a function of milage\n",
    "\n",
    "__Question__: Are these correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "# We take the mean in others\n",
    "data.pivot_table(\"pricesold\", index=\"Mileage\", aggfunc='mean').plot(ax=ax, marker='x', linestyle='none')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a5eac",
   "metadata": {},
   "source": [
    "Does it vary with year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(1, 3, sharey=True)\n",
    "# We take the mean in others\n",
    "for i, year in enumerate((2018, 2019, 2020)):\n",
    "    data.pivot_table(\"pricesold\", index=\"Mileage\", columns=['yearsold'], aggfunc='mean')[year].plot(ax=ax[i], marker='x', linestyle='none')\n",
    "    ax[i].set_xscale(\"log\")\n",
    "    ax[i].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208bb2f",
   "metadata": {},
   "source": [
    "At this point we would like to see about the role of the cars age. However, we find out there's something off with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Year']\n",
    "# We see that we have 92, 0, 20140000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642ce6c",
   "metadata": {},
   "source": [
    "We show one way of dealing with strangely formated data in the next example. In particular, we will address the problem already when reading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328e666",
   "metadata": {},
   "source": [
    "__2. People in Norway by region and age in 2022__\n",
    "\n",
    "The dateset is obtained from [SSB](https://data.ssb.no/api/v0/dataset/1076?lang=no) and we see that the encoding of the age will present some difficulty for numerics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4071d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/nor_population2022.csv\") as f:\n",
    "    ln = 0\n",
    "    while ln < 5:\n",
    "        print(next(f).strip())\n",
    "        ln += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd77a12",
   "metadata": {},
   "source": [
    "Our solution is to specify a converter for the column which uses regexp to get the age string which is converted to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# And we also get rid of some redundant data\n",
    "data = pd.read_csv(\"./data/nor_population2022.csv\", delimiter=\";\",\n",
    "                   converters={'alder': lambda x: int(re.search('(\\d+) (ar)', x).group(1))}).drop([\"ar\", \"xxx\"], axis=\"columns\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4def2",
   "metadata": {},
   "source": [
    "**Question**: Can you now fix the problem with car dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe70ce7",
   "metadata": {},
   "source": [
    "We can have a look at the populaton in different regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"region\")[\"count\"].sum().plot()\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8b3f2",
   "metadata": {},
   "source": [
    "Or breakdown the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "data.pivot_table(\"count\", index=\"alder\", columns=[\"region\"]).plot(ax=ax);\n",
    "ax.legend(bbox_to_anchor=(1.05, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ed1db",
   "metadata": {},
   "source": [
    "__3. Joining__\n",
    "\n",
    "This example is adopted from * [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas. We first download the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv\n",
    "!mv state-abbrevs.csv ./data\n",
    "!wget https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv \n",
    "!mv state-areas.csv ./data\n",
    "!wget https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv \n",
    "!mv state-population.csv ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdeedf",
   "metadata": {},
   "source": [
    "You can see that they contain data [full name -> abbreviation], [full_name -> area], [abbreviation -> full_name]. Given this we want to order the stated by population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc27ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrevs = pd.read_csv('./data/state-abbrevs.csv')\n",
    "areas = pd.read_csv('./data/state-areas.csv')\n",
    "pop = pd.read_csv('./data/state-population.csv')\n",
    "areas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(areas.columns, pop.columns, abbrevs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb772745",
   "metadata": {},
   "source": [
    "We want to build a table which has both areas and population info. To get there population should get abbreviation so that we can look up areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84853648",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(pop, abbrevs, left_on='state/region', right_on=\"abbreviation\", how=\"outer\").drop(\"abbreviation\", axis=1)\n",
    "merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69708258",
   "metadata": {},
   "source": [
    "We inspect the integrity of dataset. Something is wrong with `state/region` which is a problem if we want to map further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c91b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa018c1",
   "metadata": {},
   "source": [
    "By further inspection the missing data is for \"not the usual 50 states\" and so we just drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[merged['state'].isnull(), 'state/region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adca33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536e5a5",
   "metadata": {},
   "source": [
    "We can add to our table (left merge) the areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(merged, areas, on=\"state\", how=\"left\")\n",
    "final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3695e6",
   "metadata": {},
   "source": [
    "We use `query` (SQL like) language to get the right rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45847ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = final.query(\"year == 2012 & ages == 'total'\") #final['population']/final['area (sq. mi)']\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f2317",
   "metadata": {},
   "source": [
    "We will use the states to index into the column. By index preservation this will give us states to density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e16361",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.set_index('state', inplace=True)\n",
    "density = table['population']/table['area (sq. mi)']\n",
    "density.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "density.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f736cb",
   "metadata": {},
   "source": [
    "__Question__: Similar data can be obtained from [Eurostat](https://ec.europa.eu/eurostat/web/population-demography/demography-population-stock-balance/database). The new thing is that the format `.tsv`. Let's start parsing the population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f893edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/eustat_population.tsv\") as f:\n",
    "    ln = 0\n",
    "    while ln < 10:\n",
    "        print(next(f).strip())\n",
    "        ln += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5abf6",
   "metadata": {},
   "source": [
    "We see that the countries are subdivided. Let's figure out the reader to help here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37666160",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('./data/eustat_population.tsv', sep='\\t',\n",
    "                converters={0: lambda x: re.search('([A-Z]{2})(\\w\\w)$', x).group(1)}\n",
    "           )\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41371424",
   "metadata": {},
   "source": [
    "The next challenge is to handle the missing values which do not show with `d.isna()` as all the values are strings and the missing is \": \". How would you solve this issue? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f382e6",
   "metadata": {},
   "source": [
    "__4. Timeseries analysis__\n",
    "\n",
    "Our final examples follows Chapter 4. in PDSH. We will be looking at statics from sensors counting bikers on a bridge. We have 2 counters one for left/right side each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888577d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD;\n",
    "!mv FremontBridge.csv ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9021012",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b96ba4",
   "metadata": {},
   "source": [
    "For the next steps we will only consider the total count of bikers so we simplify the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c060c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Total', 'East', 'West']\n",
    "data.drop('East', axis=\"columns\", inplace=True)\n",
    "data.drop('West', axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865c687",
   "metadata": {},
   "source": [
    "Marking at the data we can definitely see Covid19 but also some seasonal variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()\n",
    "plt.ylabel('Hourly Bicycle Count');  # NOTE: that hour data is for every hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc6ec4",
   "metadata": {},
   "source": [
    "We can downsample the signal to reveal the variations within year better. Below we downsample to **W**eeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = data.resample('W').sum()\n",
    "# Confirmation \n",
    "weekly.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again\n",
    "weekly.index.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c78958",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly.plot(style=['-'])\n",
    "plt.ylabel('Weekly bicycle count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7e2ab",
   "metadata": {},
   "source": [
    "Recall that having time series as index enables many convience functions. For example, we can group/bin data time points by hour and reduce on the subindices giving hous an hourly mean bike usage. It appears to correlate well with rush hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c026f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time = data.groupby(data.index.time).mean()\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)    # 3600 * 24, every 4 hour\n",
    "by_time.plot(xticks=hourly_ticks, style=['-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98d863",
   "metadata": {},
   "source": [
    "We can repeat the same exercise, this time breaking up the data by hours to reveal that bikes are probably most used for commuting to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aef6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_weekday = data.groupby(data.index.dayofweek).mean()\n",
    "by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "by_weekday.plot(style=['-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682ace3",
   "metadata": {},
   "source": [
    "Another look at the same thing using `pivot_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "Z = data.pivot_table(\"Total\", index=data.index.weekday, columns=[data.index.time]).values\n",
    "\n",
    "plt.pcolor(Z, norm=colors.LogNorm(vmin=Z.min(), vmax=Z.max()), cmap='inferno')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa283b",
   "metadata": {},
   "source": [
    "The heatmap suggests that there are 2 peaks in weekdays and a single one during the weekend. We can varify this by essentially \n",
    "collapsing/averaging the plot in the vertical direction separately for the two categories of days. Let's compute the mask for each day in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a2809",
   "metadata": {},
   "source": [
    "Using the mask we can group the data first by the day catogory and then by hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c47eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time = data.groupby([weekend, data.index.time]).mean()\n",
    "by_time.index[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb693ca",
   "metadata": {},
   "source": [
    "Finally we have the two collapsed plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e83222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "by_time.loc['Weekday'].plot(ax=ax[0], title='Weekdays', xticks=hourly_ticks, style=['-'])\n",
    "by_time.loc['Weekend'].plot(ax=ax[1], title='Weekends', xticks=hourly_ticks, style=['-'], color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10edfc2",
   "metadata": {},
   "source": [
    "__Question__: Head on to [Oslo Bysykkel web](https://oslobysykkel.no/apne-data/historisk) and get the trip data (as CSV). In the last years the students analyzed the trips by distance (see the IN3110 course webpage). What about the distribution of trips by the average speed? Can you use this to infer if the trip was going (on average) uphill or downhill?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://data.urbansharing.com/oslobysykkel.no/trips/v1/2022/09.csv\n",
    "! mv 09.csv ./data/oslo_bike_september_2022.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/oslo_bike_september_2022.csv\") as f:\n",
    "    print(next(f).strip())\n",
    "    print(next(f).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df46ea",
   "metadata": {},
   "source": [
    "At this point we know that time info should be parsed with dates. For duration (in seconds) we will have ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f6b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/oslo_bike_september_2022.csv', parse_dates=['started_at', 'ended_at'], dtype={'duration': int})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221f55d",
   "metadata": {},
   "source": [
    "What is the station with most departures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['start_station_id'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac5ac0",
   "metadata": {},
   "source": [
    "We will approximate the trip distance by measing it as the geodesic between start and end points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi, sin, cos, asin, sqrt\n",
    "\n",
    "def trip_distance(row):\n",
    "    \"\"\"As the crow flies\"\"\"\n",
    "    lat_station = row[\"start_station_latitude\"]\n",
    "    lon_station = row[\"start_station_longitude\"]\n",
    "\n",
    "    lat_sentrum = row[\"end_station_latitude\"]\n",
    "    lon_sentrum = row[\"end_station_longitude\"]\n",
    "\n",
    "    degrees = pi / 180  # convert degrees to radians\n",
    "    a = (\n",
    "        0.5\n",
    "        - (cos((lat_sentrum - lat_station) * degrees) / 2)\n",
    "        + (\n",
    "            cos(lat_sentrum * degrees)\n",
    "            * cos(lat_station * degrees)\n",
    "            * (1 - cos((lon_station - lon_sentrum) * degrees))\n",
    "            / 2\n",
    "        )\n",
    "    )\n",
    "    # We have the return value in meters\n",
    "    return 12742_000 * asin(sqrt(a))  # 2 * R * asin...\n",
    "\n",
    "distance = data.apply(trip_distance, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246782f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = data['duration']\n",
    "speed = distance/duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a820f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "speed.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4b460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e436fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92ea19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34859711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cbccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3e770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e13059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630f156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ab9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07429b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipywidgets import HTML\n",
    "#from ipyleaflet import Map, Marker, basemaps, basemap_to_tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6d289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_id, idx = np.unique(data['end_station_id'], return_index=True)\n",
    "end = data.loc[idx, 'end_station_name']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87530212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_id, idx = np.unique(data['start_station_id'], return_index=True)\n",
    "start = data.loc[start_id]['start_station_name']\n",
    "start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
